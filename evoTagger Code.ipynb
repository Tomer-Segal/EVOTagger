{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JulNo_-_kUiHDNkV_dQ1gMGwjlWKrg1a","timestamp":1760992074845},{"file_id":"1bnFcB2pu-6npiDgC4UfdPF3CHI3xCYYA","timestamp":1760203416759},{"file_id":"1MM17aQFzE8YdMVxbqHuOf3rwO5bgFPYC","timestamp":1760182878549},{"file_id":"1hfcoAwJpToeqOReAfLgcZ-AaB-bofu_P","timestamp":1760090596058}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPNgwRm85D66Al6ycA5joGf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### I run the code on High-RAM (80GB) A100 GPU on google colab"],"metadata":{"id":"DdvTC2792m5q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLW8k_lvLCUp","collapsed":true},"outputs":[],"source":["%pip -q uninstall -y transformers tokenizers huggingface-hub peft trl optimum sentence-transformers || true\n","\n","%pip -q install \"transformers==4.45.2\" \"accelerate>=0.29,<1\" \"safetensors\"\n","\n","%pip -q install -U \"huggingface_hub>=0.23\"\n","\n","%pip -q install \"torch==2.5.1\" \"torchvision==0.20.1\" \"torchaudio==2.5.1\" --index-url https://download.pytorch.org/whl/cu124\n","%pip -q install \"flash-attn==2.7.4.post1\" --no-build-isolation\n","\n","%pip -q install -U evo-model\n","\n","!python -m pip check"]},{"cell_type":"code","source":["import torch, gc, difflib\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from evo.scoring import prepare_batch\n","from evo import Evo\n","import ast  # For parsing label strings\n","from datetime import datetime\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","import sys, os\n","import importlib\n","import pandas as pd\n","\n","def free_gpu(*objs):\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        torch.cuda.ipc_collect()\n","    print(\"Freed Python GC and CUDA caches.\")"],"metadata":{"collapsed":true,"id":"m2NfRkugNY5T","executionInfo":{"status":"ok","timestamp":1761150239208,"user_tz":-180,"elapsed":2848,"user":{"displayName":"Tomer Segal","userId":"00501949809997728223"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Mount Data"],"metadata":{"id":"kqr9YF46Nxk6"}},{"cell_type":"code","source":["drive.mount('/content/drive/')\n","\n","BASE = \"/content/drive/MyDrive/EvoTagger-files\"\n","CSV_PATH_TRAIN = f\"{BASE}/ecoli_train_12k.csv\"\n","CSV_PATH_VAL = f\"{BASE}/ecoli_val_750.csv\"\n","CSV_PATH_TEST = f\"{BASE}/ecoli_test_5k.csv\"\n","CHECKPOINT_PATH = f\"{BASE}/checkpoints\"\n","\n","if BASE not in sys.path:\n","    sys.path.append(BASE)"],"metadata":{"id":"9OGb7wE0NwZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data Prepration"],"metadata":{"id":"rKi04Why_B26"}},{"cell_type":"code","source":["NUM_OF_TRAIN_SEQ = 12000\n","NUM_OF_VAL_SEQ = 750\n","NUM_OF_TEST_SEQ = 5000\n","\n","print(\"Loading data from data/data_with_labels.csv...\")\n","train_df = pd.read_csv(CSV_PATH_TRAIN)\n","val_df = pd.read_csv(CSV_PATH_VAL)\n","train_df = train_df[:NUM_OF_TRAIN_SEQ]\n","val_df = val_df[:NUM_OF_VAL_SEQ]\n","print(f\"✓ Loaded {len(train_df)}, {len(val_df)} sequences\")\n","\n","\n","# ============================================================================\n","# Dataset and Collate Function\n","# ============================================================================\n","\n","class DNADataset(Dataset):\n","    \"\"\"\n","    Dataset for DNA error correction.\n","    Uses pre-computed labels from CSV (no need to regenerate).\n","    \"\"\"\n","    def __init__(self, noisy_list, clean_list, fine_labels_list, coarse_labels_list):\n","        \"\"\"\n","        Args:\n","            noisy_list: List of noisy DNA sequences\n","            clean_list: List of clean DNA sequences\n","            fine_labels_list: List of pre-computed fine-grained labels\n","            coarse_labels_list: List of pre-computed coarse-grained labels\n","        \"\"\"\n","        self.noisy = noisy_list\n","        self.clean = clean_list\n","        self.fine_labels = fine_labels_list\n","        self.coarse_labels = coarse_labels_list\n","\n","    def __len__(self):\n","        return len(self.noisy)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        Returns:\n","            noisy_seq: DNA sequence string\n","            fine_labels: Tensor of fine-grained labels\n","            coarse_labels: Tensor of coarse-grained labels\n","        \"\"\"\n","        return (\n","            self.noisy[i],\n","            torch.tensor(self.fine_labels[i], dtype=torch.long),\n","            torch.tensor(self.coarse_labels[i], dtype=torch.long)\n","        )\n","\n","\n","def create_collate_fn(tokenizer, device):\n","    \"\"\"\n","    Factory function to create collate_fn with tokenizer and device bound.\n","    CharLevelTokenizer uses .tokenize() method, not __call__.\n","    \"\"\"\n","    def collate_fn(batch):\n","        # batch: list of (noisy_str, fine_labels, coarse_labels)\n","        seqs = [b[0] for b in batch]\n","        fine = [b[1] for b in batch]\n","        coarse = [b[2] for b in batch]\n","\n","        # Tokenize sequences\n","        # CharLevelTokenizer.tokenize(seq) returns list of token IDs\n","        tokenized = [tokenizer.tokenize(seq) for seq in seqs]\n","\n","        # Pad to max length in batch\n","        max_len = max(len(t) for t in tokenized)\n","        pad_id = tokenizer.pad_id\n","\n","        input_ids = torch.full((len(batch), max_len), pad_id, dtype=torch.long, device=device)\n","        seq_lengths = []\n","\n","        for i, tokens in enumerate(tokenized):\n","            seq_len = len(tokens)\n","            input_ids[i, :seq_len] = torch.tensor(tokens, dtype=torch.long, device=device)\n","            seq_lengths.append(seq_len)\n","\n","        # Pad labels\n","        pad_val = -100  # ignore_index for CE\n","        fine_pad = torch.full((len(batch), max_len), pad_val, dtype=torch.long, device=device)\n","        coarse_pad = torch.full((len(batch), max_len), pad_val, dtype=torch.long, device=device)\n","\n","        for i, (f, c, L) in enumerate(zip(fine, coarse, seq_lengths)):\n","            fine_pad[i, :L] = f.to(device)\n","            coarse_pad[i, :L] = c.to(device)\n","\n","        return input_ids, fine_pad, coarse_pad\n","\n","    return collate_fn"],"metadata":{"id":"EjGJ2Upm_En3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model"],"metadata":{"id":"vwbaFnjz-vWm"}},{"cell_type":"code","source":["FINE = {\n","    \"KEEP\": 0,\n","    \"REPLACE_A\": 1, \"REPLACE_C\": 2, \"REPLACE_G\": 3, \"REPLACE_T\": 4,\n","}\n","COARSE_MAP = {0: 0, 1: 1, 2: 1, 3: 1, 4: 1}  # KEEP=0, REPLACE=1\n","\n","class CustomEmbedding(nn.Module):\n","    \"\"\"Passthrough layer - returns input unchanged.\"\"\"\n","    def unembed(self, u):\n","        return u\n","\n","\n","class EvoFeaturizer(nn.Module):\n","    \"\"\"\n","    Wrap Evo and expose (B, L, D) features.\n","\n","    Extracts rich DNA sequence embeddings from the frozen Evo model.\n","    \"\"\"\n","    def __init__(self, evo_backbone):\n","        super().__init__()\n","        self.evo = evo_backbone\n","\n","        # This makes the model return hidden states instead of logits\n","        self.evo.unembed = CustomEmbedding()\n","\n","    def forward(self, input_ids):\n","        # Forward through Evo with modified unembed (returns hidden states)\n","        with torch.no_grad():\n","            hidden_states, _ = self.evo(input_ids)  # Fallback for backward compatibility\n","\n","            # Convert from bfloat16 to float32 for compatibility with LSTM\n","            hidden_states = hidden_states.float()  # (B, L, 4096) in float32\n","\n","        return hidden_states  # Output: (B, L, D=4096)\n","\n","class DNATagger(nn.Module):\n","    \"\"\"\n","    Hierarchical character-level tagger for DNA error correction - SIMPLIFIED VERSION.\n","\n","    Predicts edit operations at each position:\n","    - Fine-grained: 5 classes (KEEP, REPLACE_A, REPLACE_C, REPLACE_G, REPLACE_T)\n","    - Coarse-grained: 2 categories (KEEP, REPLACE)\n","    \"\"\"\n","    def __init__(self, D, num_fine=5):\n","        super().__init__()\n","\n","        # Layer normalization for input features (helps convergence)\n","        self.input_norm = nn.LayerNorm(D)\n","\n","        # BiLSTM for context modeling\n","        # Input: D, Hidden: D//2, Bidirectional → Output: 2*(D//2) = D\n","        self.ctx = nn.LSTM(D, D//2, num_layers=1, bidirectional=True, batch_first=True)\n","\n","        # Layer normalization after LSTM (helps convergence)\n","        self.ctx_norm = nn.LayerNorm(D)\n","\n","        # Fine-grained prediction head (5 classes)\n","        self.head_fine = nn.Linear(D, num_fine)\n","\n","        # Hierarchical grouping: fine labels → coarse categories\n","        # SIMPLIFIED: Only 2 coarse categories\n","        # Group 0: KEEP (label 0)\n","        # Group 1: REPLACE (labels 1, 2, 3, 4 = REPLACE_A, C, G, T)\n","        self.groups = {\n","            0: [0],           # KEEP\n","            1: [1, 2, 3, 4]   # REPLACE_{A,C,G,T}\n","        }\n","\n","    def forward(self, H):\n","        # Step 1: Normalize input features (stabilizes training)\n","        H = self.input_norm(H)                           # (B, L, D)\n","\n","        # Step 2: BiLSTM for bidirectional context\n","        H2, _ = self.ctx(H)                              # (B, L, D) - context features\n","\n","        # Step 3: Normalize after LSTM (helps gradient flow)\n","        H2 = self.ctx_norm(H2)                           # (B, L, D)\n","\n","        # Step 4: Predict fine-grained labels\n","        fine_logits = self.head_fine(H2)                 # (B, L, 5)\n","\n","        # Step 5: Aggregate to coarse labels via log-sum-exp (parameter-free!)\n","        coarse_logits = []\n","\n","        for gid in [0, 1]:\n","            # Get indices for this group\n","            # gid=0: [0] (KEEP)\n","            # gid=1: [1,2,3,4] (REPLACE_A, REPLACE_C, REPLACE_G, REPLACE_T)\n","            idx = torch.tensor(self.groups[gid], device=H.device)\n","\n","            # Select logits for this group: (B, L, 5) → (B, L, group_size)\n","            # Then logsumexp over last dim: (B, L, group_size) → (B, L, 1)\n","            group_logits = fine_logits.index_select(-1, idx)  # (B, L, group_size)\n","            aggregated = torch.logsumexp(group_logits, dim=-1, keepdim=True)  # (B, L, 1)\n","            coarse_logits.append(aggregated)\n","\n","        # Concatenate 2 groups: [(B,L,1), (B,L,1)] → (B,L,2)\n","        coarse_logits = torch.cat(coarse_logits, dim=-1)  # (B, L, 2)\n","\n","        return fine_logits, coarse_logits  # (B, L, 5), (B, L, 2)"],"metadata":{"id":"OElm_EVy-vC2","executionInfo":{"status":"ok","timestamp":1761150407744,"user_tz":-180,"elapsed":32,"user":{"displayName":"Tomer Segal","userId":"00501949809997728223"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"TAhuwHnvNQAH"}},{"cell_type":"code","source":["# ============================================================================\n","# Training Script\n","# ============================================================================\n","free_gpu()\n","\n","\n","def main():\n","    # Device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\\n\")\n","\n","    # Load Evo model\n","    print(\"Loading Evo model...\")\n","    evo_model = Evo('evo-1.5-8k-base')\n","\n","    # Evo wrapper handles device placement internally\n","    tokenizer = evo_model.tokenizer\n","    model_backbone = evo_model.model.to(device)\n","    model_backbone.eval()\n","\n","    # Create models\n","    # NOTE: evo-1-8k-base has hidden dimension 4096\n","    print(\"Creating models...\")\n","    featurizer = EvoFeaturizer(model_backbone).to(device)\n","    tagger = DNATagger(D=4096, num_fine=5).to(device)  # 5 classes: KEEP + 4 REPLACE types\n","    featurizer.eval() #freeze evo model\n","\n","    # Show mutation statistics\n","    train_mutations = train_df['num_mutations'].sum()\n","    val_mutations = val_df['num_mutations'].sum()\n","    print(f\"\\nMutation statistics:\")\n","    print(f\"  Train mutations: {train_mutations} ({train_df['num_mutations'].mean():.1f} per seq)\")\n","    print(f\"  Val mutations: {val_mutations} ({val_df['num_mutations'].mean():.1f} per seq)\")\n","    print(f\"  💡 Training data has {train_df['num_mutations'].mean():.0f}x more mutations per sequence!\")\n","\n","    # Parse pre-computed labels from CSV (they're stored as strings)\n","    print(\"\\nParsing labels...\")\n","    train_df['fine_labels_parsed'] = train_df['fine_labels'].apply(ast.literal_eval)\n","    train_df['coarse_labels_parsed'] = train_df['coarse_labels'].apply(ast.literal_eval)\n","    val_df['fine_labels_parsed'] = val_df['fine_labels'].apply(ast.literal_eval)\n","    val_df['coarse_labels_parsed'] = val_df['coarse_labels'].apply(ast.literal_eval)\n","    print(\"✓ Labels parsed\")\n","\n","    # Calculate class weights based on TRAINING data only\n","    from collections import Counter\n","    print(\"\\nAnalyzing training class distribution...\")\n","    all_fine_labels = []\n","    all_coarse_labels = []\n","    for fine_labels, coarse_labels in zip(train_df['fine_labels_parsed'], train_df['coarse_labels_parsed']):\n","        all_fine_labels.extend(fine_labels)\n","        all_coarse_labels.extend(coarse_labels)\n","\n","    fine_label_counts = Counter(all_fine_labels)\n","    coarse_label_counts = Counter(all_coarse_labels)\n","    total_labels = len(all_fine_labels)\n","\n","    # Calculate inverse frequency weights for COARSE-grained labels (2 classes)\n","    coarse_class_weights = torch.zeros(2)\n","    for label in range(2):\n","        count = coarse_label_counts.get(label, 1)\n","        coarse_class_weights[label] = total_labels / (2 * count)\n","\n","    # Apply sqrt dampening for stable gradients with extreme imbalance\n","    coarse_class_weights = torch.sqrt(coarse_class_weights)\n","    coarse_class_weights = coarse_class_weights.to(device)\n","\n","    # No split needed - we loaded separate train/val datasets!\n","    print(f\"✓ Train={len(train_df)}, Val={len(val_df)}\\n\")\n","\n","    train_noisy = train_df['noisy'].tolist()\n","    train_clean = train_df['clean'].tolist()\n","    train_fine = train_df['fine_labels_parsed'].tolist()\n","    train_coarse = train_df['coarse_labels_parsed'].tolist()\n","\n","    val_noisy = val_df['noisy'].tolist()\n","    val_clean = val_df['clean'].tolist()\n","    val_fine = val_df['fine_labels_parsed'].tolist()\n","    val_coarse = val_df['coarse_labels_parsed'].tolist()\n","\n","    print(f\"✓ Split: Train={len(train_noisy)}, Val={len(val_noisy)}\\n\")\n","\n","    # Create datasets (using pre-computed labels!)\n","    train_dataset = DNADataset(train_noisy, train_clean, train_fine, train_coarse)\n","    val_dataset = DNADataset(val_noisy, val_clean, val_fine, val_coarse)\n","\n","    batch_size = 16\n","    num_epochs = 15\n","    collate_fn = create_collate_fn(tokenizer, device)\n","    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n","    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","    optimizer = torch.optim.AdamW(tagger.parameters(), lr=1e-4, weight_decay=0.01)\n","\n","    print('Save to Drive')\n","    # Save training parameters to txt file\n","    with open(f'{CHECKPOINT_PATH}/training_params.txt', 'w') as f:\n","        f.write(\"=\"*80 + \"\\n\")\n","        f.write(\"TRAINING PARAMETERS\\n\")\n","        f.write(\"=\"*80 + \"\\n\\n\")\n","        f.write(f\"Model: DNATagger with EvoFeaturizer\\n\")\n","        f.write(f\"Evo Model: evo-1.5-8k-base\\n\")\n","        f.write(f\"Device: {device}\\n\\n\")\n","        f.write(f\"Dataset:\\n\")\n","        f.write(f\"  Train sequences: {len(train_df)}\\n\")\n","        f.write(f\"  Val sequences: {len(val_df)}\\n\")\n","        f.write(f\"  Train mutations: {train_df['num_mutations'].sum()}\\n\")\n","        f.write(f\"  Val mutations: {val_df['num_mutations'].sum()}\\n\")\n","        f.write(f\"  Avg mutations/seq (train): {train_df['num_mutations'].mean():.1f}\\n\\n\")\n","        f.write(f\"Training:\\n\")\n","        f.write(f\"  Epochs: {num_epochs}\\n\")\n","        f.write(f\"  Batch size: {batch_size}\\n\")\n","        f.write(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.6f}\\n\")\n","        f.write(f\"  Optimizer: AdamW (weight_decay=0.01)\\n\")\n","        f.write(f\"Class Weights:\\n\")\n","        f.write(f\"  Coarse (2 classes): {coarse_class_weights.cpu().tolist()}\\n\")\n","        f.write(\"=\"*80 + \"\\n\")\n","\n","    print(\"=\"*80)\n","    print(\"STARTING TRAINING\")\n","    print(\"=\"*80)\n","    print(f\"Epochs: {num_epochs}\")\n","    print(f\"Batch size: {batch_size}\")\n","    print(f\"Train batches: {len(train_loader)}\")\n","    print(f\"Val batches: {len(val_loader)}\")\n","    print(f\"Initial LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","    print(f\"✓ Saving to: checkpoints/\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    best_val_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print(f\"\\n{'='*80}\")\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(f\"{'='*80}\")\n","\n","        # Train\n","        tagger.train()\n","        train_loss = 0.0\n","        train_mutation_loss = 0.0\n","        train_coarse_loss = 0.0\n","\n","        # Training progress bar\n","        train_pbar = tqdm(train_loader, desc='Training', leave=False)\n","        for input_ids, fine_y, coarse_y in train_pbar:\n","            # Get features from frozen Evo\n","            with torch.no_grad():\n","                H = featurizer(input_ids)\n","\n","            # Get predictions\n","            fine_logits, coarse_logits = tagger(H)\n","\n","            # Calculate losses\n","            B, L, Kf = fine_logits.shape\n","\n","            mutation_mask = (fine_y != 0) & (fine_y != -100)\n","            if mutation_mask.sum() > 0:\n","                mutation_loss = F.cross_entropy(\n","                    fine_logits[mutation_mask],\n","                    fine_y[mutation_mask]\n","                )\n","            else:\n","                mutation_loss = 0\n","\n","            coarse_loss = F.cross_entropy(\n","                coarse_logits.view(B*L, 2),\n","                coarse_y.view(B*L),\n","                weight=coarse_class_weights,  # Sqrt dampening\n","                ignore_index=-100\n","            )\n","\n","            # Combined loss\n","            loss = 0.5 * mutation_loss + 0.5 * coarse_loss\n","\n","            # Backprop\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Track losses\n","            train_loss += loss.item()\n","            train_mutation_loss += mutation_loss.item()\n","            train_coarse_loss += coarse_loss.item()\n","\n","            # Update progress bar\n","            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        train_loss /= len(train_loader)\n","        train_mutation_loss /= len(train_loader)\n","        train_coarse_loss /= len(train_loader)\n","\n","        # Validate\n","        tagger.eval()\n","        val_loss = 0.0\n","        val_mutation_loss = 0.0\n","        val_coarse_loss = 0.0\n","        val_correct = 0\n","        val_total = 0\n","\n","        # Track mutation-specific metrics\n","        mutation_correct = 0  # Correct predictions for non-KEEP labels\n","        mutation_total = 0    # Total non-KEEP labels\n","        per_class_correct = [0] * 5  # Correct per class (5 classes)\n","        per_class_total = [0] * 5    # Total per class (5 classes)\n","\n","        # Validation progress bar\n","        val_pbar = tqdm(val_loader, desc='Validation', leave=False)\n","        with torch.no_grad():\n","            for input_ids, fine_y, coarse_y in val_pbar:\n","                H = featurizer(input_ids)\n","                fine_logits, coarse_logits = tagger(H)\n","\n","                B, L, Kf = fine_logits.shape\n","\n","\n","                mutation_mask = (fine_y != 0) & (fine_y != -100)\n","                if mutation_mask.sum() > 0:\n","                    mutation_loss = F.cross_entropy(\n","                        fine_logits[mutation_mask],\n","                        fine_y[mutation_mask]\n","                    )\n","                else:\n","                    mutation_loss = 0\n","\n","                coarse_loss = F.cross_entropy(\n","                    coarse_logits.view(B*L, 2),\n","                    coarse_y.view(B*L),\n","                    weight=coarse_class_weights,\n","                    ignore_index=-100\n","                )\n","\n","                # Combined loss\n","                loss = 0.5 * mutation_loss + 0.5 * coarse_loss\n","                val_loss += loss.item()\n","                val_mutation_loss += mutation_loss.item()\n","                val_coarse_loss += coarse_loss.item()\n","\n","                # Calculate overall accuracy\n","                fine_pred = fine_logits.argmax(dim=-1)\n","                mask = (fine_y != -100)\n","                val_correct += ((fine_pred == fine_y) & mask).sum().item()\n","                val_total += mask.sum().item()\n","\n","                # Calculate mutation-specific accuracy (non-KEEP only)\n","                # KEEP = class 0, mutations = classes 1-4\n","                mutation_mask = mask & (fine_y != 0)  # Valid positions that are NOT KEEP\n","                if mutation_mask.sum() > 0:\n","                    mutation_correct += ((fine_pred == fine_y) & mutation_mask).sum().item()\n","                    mutation_total += mutation_mask.sum().item()\n","\n","                # Per-class accuracy\n","                for class_idx in range(5):\n","                    class_mask = mask & (fine_y == class_idx)\n","                    if class_mask.sum() > 0:\n","                        per_class_correct[class_idx] += ((fine_pred == fine_y) & class_mask).sum().item()\n","                        per_class_total[class_idx] += class_mask.sum().item()\n","\n","                # Update progress bar\n","                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        val_loss /= len(val_loader)\n","        val_mutation_loss /= len(val_loader)\n","        val_coarse_loss /= len(val_loader)\n","        val_acc = val_correct / val_total if val_total > 0 else 0\n","\n","        # Calculate mutation-specific accuracy\n","        mutation_acc = mutation_correct / mutation_total if mutation_total > 0 else 0\n","\n","        # Calculate per-class accuracies\n","        per_class_acc = []\n","        for i in range(5):\n","            if per_class_total[i] > 0:\n","                per_class_acc.append(per_class_correct[i] / per_class_total[i])\n","            else:\n","                per_class_acc.append(0.0)\n","\n","        # Track best model (based on mutation accuracy, not overall!)\n","        is_best = mutation_acc > best_val_acc\n","        if is_best:\n","            best_val_acc = mutation_acc\n","\n","        # Print detailed epoch summary\n","        print(f\"\\nTRAIN:\")\n","        print(f\"  Total Loss:  {train_loss:.4f}\")\n","        print(f\"  Mutation Loss:   {train_mutation_loss:.4f}\")\n","        print(f\"  Coarse Loss: {train_coarse_loss:.4f}\")\n","        print(f\"\\nVALIDATION:\")\n","        print(f\"  Total Loss:  {val_loss:.4f}\")\n","        print(f\"  Mutation Loss:   {val_mutation_loss:.4f}\")\n","        print(f\"  Coarse Loss: {val_coarse_loss:.4f}\")\n","        print(f\"  Overall Accuracy:  {val_acc:.4f} ({val_correct}/{val_total})\")\n","        print(f\"  ⭐ MUTATION Accuracy: {mutation_acc:.4f} ({mutation_correct}/{mutation_total}) {'🌟 NEW BEST!' if is_best else ''}\")\n","        print(f\"  Best Mutation Acc:  {best_val_acc:.4f}\")\n","\n","        # Show per-class breakdown (classes with >0 samples)\n","        print(f\"\\n  Per-Class Accuracy:\")\n","        class_names = {\n","            0: 'KEEP',\n","            1: 'REPLACE_A', 2: 'REPLACE_C', 3: 'REPLACE_G', 4: 'REPLACE_T'\n","        }\n","        for i in range(5):\n","            if per_class_total[i] > 0:\n","                name = class_names.get(i, f'Class_{i}')\n","                acc = per_class_acc[i]\n","                count = per_class_total[i]\n","                print(f\"    {name:<12}: {acc:.3f} ({per_class_correct[i]}/{count})\")\n","\n","        # Save model checkpoint\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'model_state_dict': tagger.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'train_loss': train_loss,\n","            'val_loss': val_loss,\n","            'mutation_acc': mutation_acc,\n","            'val_acc': val_acc,\n","            'best_val_acc': best_val_acc,\n","            'per_class_acc': per_class_acc,\n","        }\n","        checkpoint_path = f'{CHECKPOINT_PATH}/model_epoch.pt'\n","        torch.save(checkpoint, checkpoint_path)\n","\n","        # Also save best model separately\n","        if is_best:\n","            torch.save(checkpoint, f'{CHECKPOINT_PATH}/model_best.pt')\n","            print(f\"\\n  💾 Saved best model (mutation_acc={mutation_acc:.4f})\")\n","\n","    # Training complete - show summary\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"TRAINING COMPLETE!\")\n","    print(\"=\"*80)\n","    print(f\"Best Mutation Accuracy: {best_val_acc:.4f}\")\n","    print(f\"Final Overall Accuracy: {val_acc:.4f}\")\n","    print(f\"Final Mutation Accuracy: {mutation_acc:.4f}\")\n","    print(f\"Final Train Loss: {train_loss:.4f}\")\n","    print(f\"Final Val Loss: {val_loss:.4f}\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    # Save model\n","    print(\"Saving model...\")\n","    torch.save(tagger.state_dict(), 'simple_model.pt')\n","    print(\"✓ Saved to simple_model.pt (local)\")\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"Next step: Run simple_inference.py to test the model!\")\n","    print(\"=\"*80)\n","\n","main()"],"metadata":{"id":"kL1A8p3wNRcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test"],"metadata":{"id":"B8_k0ZKX0hBc"}},{"cell_type":"code","source":["def load_model(checkpoint_path, device):\n","    \"\"\"Load trained model from checkpoint.\"\"\"\n","    print(f\"Loading model from {checkpoint_path}...\")\n","\n","    # Load Evo\n","    print(\"Loading Evo model...\")\n","    evo_model = Evo('evo-1.5-8k-base')\n","    tokenizer = evo_model.tokenizer\n","    model_backbone = evo_model.model.to(device)\n","    model_backbone.eval()\n","\n","    # Create models\n","    featurizer = EvoFeaturizer(model_backbone).to(device)\n","    tagger = DNATagger(D=4096, num_fine=5).to(device)\n","    featurizer.eval()\n","\n","    # Load checkpoint\n","    checkpoint = torch.load(checkpoint_path, map_location=device)\n","    tagger.load_state_dict(checkpoint['model_state_dict'])\n","    tagger.eval()\n","\n","    print(f\"✓ Model loaded from epoch {checkpoint['epoch']}\")\n","    print(f\"  Training mutation acc: {checkpoint['mutation_acc']:.4f}\")\n","    print(f\"  Training overall acc: {checkpoint['val_acc']:.4f}\\n\")\n","\n","    return featurizer, tagger, tokenizer\n","\n","def predict_sequence(sequence, featurizer, tagger, tokenizer, device):\n","    \"\"\"\n","    Predict corrections for a single sequence.\n","\n","    Returns:\n","        fine_predictions: List of predicted labels (0-4)\n","        fine_probs: List of confidence scores\n","    \"\"\"\n","    # Tokenize\n","    tokens = tokenizer.tokenize(sequence)\n","    input_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n","\n","    # Get features and predictions\n","    with torch.no_grad():\n","        H = featurizer(input_ids)\n","        fine_logits, coarse_logits = tagger(H)\n","\n","    # Get predictions and probabilities\n","    fine_probs = F.softmax(fine_logits[0], dim=-1)  # (L, 5)\n","    fine_preds = fine_logits[0].argmax(dim=-1)  # (L,)\n","\n","    return fine_preds.cpu().tolist(), fine_probs.cpu()\n","\n","def test_from_csv(csv_path, model_path, device, num_samples=10):\n","    \"\"\"\n","    Test model on sequences from CSV file.\n","\n","    Args:\n","        csv_path: Path to CSV with test data\n","        model_path: Path to model checkpoint\n","        device: CUDA or CPU\n","        num_samples: Number of sequences to test\n","    \"\"\"\n","    print(\"=\"*80)\n","    print(\"DNA ERROR CORRECTION - TEST SCRIPT\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    # Load model\n","    featurizer, tagger, tokenizer = load_model(model_path, device)\n","\n","    # Load test data\n","    print(f\"Loading test data from {csv_path}...\")\n","    df = pd.read_csv(csv_path)\n","    print(f\"✓ Loaded {len(df)} sequences\\n\")\n","\n","    # Test on random samples\n","    import random\n","    test_indices = random.sample(range(len(df)), min(num_samples, len(df)))\n","\n","    label_names = {\n","        0: 'KEEP',\n","        1: 'REPLACE_A',\n","        2: 'REPLACE_C',\n","        3: 'REPLACE_G',\n","        4: 'REPLACE_T'\n","    }\n","\n","    total_correct = 0\n","    total_positions = 0\n","    mutation_correct = 0\n","    mutation_total = 0\n","    per_class_correct = [0] * 5  # Per-class accuracy tracking\n","    per_class_total = [0] * 5\n","\n","    print(\"=\"*80)\n","    print(f\"TESTING {num_samples} SEQUENCES\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    for idx in test_indices:\n","        row = df.iloc[idx]\n","        noisy_seq = row['noisy']\n","        clean_seq = row['clean']\n","        true_fine = ast.literal_eval(row['fine_labels'])\n","\n","        print(f\"Sequence {idx}:\")\n","        print(f\"  Length: {len(noisy_seq)} bases\")\n","        print(f\"  Mutations: {row['num_mutations']}\")\n","\n","        # Get predictions\n","        pred_fine, pred_probs = predict_sequence(noisy_seq, featurizer, tagger, tokenizer, device)\n","\n","        # Find mutation positions\n","        mutation_positions = [i for i, label in enumerate(true_fine) if label != 0]\n","\n","        if mutation_positions:\n","            print(f\"\\n  Mutation Details:\")\n","            for pos in mutation_positions[:5]:  # Show first 5\n","                if pos < len(pred_fine):\n","                    true_label = true_fine[pos]\n","                    pred_label = pred_fine[pos]\n","                    confidence = pred_probs[pos, pred_label].item()\n","\n","                    correct = \"✓\" if pred_label == true_label else \"✗\"\n","                    print(f\"    Position {pos}: {noisy_seq[pos]} → {clean_seq[pos]}\")\n","                    print(f\"      True:  {label_names[true_label]}\")\n","                    print(f\"      Pred:  {label_names[pred_label]} ({confidence:.2%} conf) {correct}\")\n","\n","            if len(mutation_positions) > 5:\n","                print(f\"    ... and {len(mutation_positions) - 5} more mutations\")\n","\n","        # Calculate accuracy\n","        seq_len = min(len(true_fine), len(pred_fine))\n","        correct = sum(1 for i in range(seq_len) if pred_fine[i] == true_fine[i])\n","        total_correct += correct\n","        total_positions += seq_len\n","\n","        # Per-class and mutation-specific accuracy\n","        for i in range(seq_len):\n","            true_label = true_fine[i]\n","            pred_label = pred_fine[i]\n","\n","            # Track per-class stats\n","            per_class_total[true_label] += 1\n","            if pred_label == true_label:\n","                per_class_correct[true_label] += 1\n","\n","            # Track mutation stats\n","            if true_label != 0:  # Non-KEEP (mutation)\n","                mutation_total += 1\n","                if pred_label == true_label:\n","                    mutation_correct += 1\n","\n","        print(f\"\\n  Accuracy: {correct}/{seq_len} = {100*correct/seq_len:.1f}%\")\n","        print(\"  \" + \"-\"*76 + \"\\n\")\n","\n","    # Overall statistics\n","    print(\"=\"*80)\n","    print(\"TEST SUMMARY\")\n","    print(\"=\"*80)\n","    print(f\"  Overall Accuracy:  {100*total_correct/total_positions:.2f}% ({total_correct}/{total_positions})\")\n","    if mutation_total > 0:\n","        print(f\"  Mutation Accuracy: {100*mutation_correct/mutation_total:.2f}% ({mutation_correct}/{mutation_total})\")\n","    else:\n","        print(f\"  Mutation Accuracy: N/A (no mutations in test set)\")\n","\n","    # Per-class accuracy breakdown\n","    print(f\"\\n  Per-Class Accuracy:\")\n","    for class_idx in range(5):\n","        if per_class_total[class_idx] > 0:\n","            name = label_names[class_idx]\n","            acc = per_class_correct[class_idx] / per_class_total[class_idx]\n","            count = per_class_total[class_idx]\n","            print(f\"    {name:<12}: {acc:.3f} ({per_class_correct[class_idx]}/{count})\")\n","\n","    print(\"=\"*80 + \"\\n\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\\n\")\n","\n","model_path_to_test = f'{BASE}/model_epoch.pt'\n","\n","test_from_csv(CSV_PATH_TEST, model_path_to_test, device, NUM_OF_TEST_SEQ)"],"metadata":{"id":"C-kySftpl8bn"},"execution_count":null,"outputs":[]}]}